<!DOCTYPE html>
<html> 
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="generator" content="RocketCake">
	<title></title>
	<link rel="stylesheet" type="text/css" href="index_html.css">
</head>
<body>
<div class="textstyle1">
<div id="container_6847f1d0"><div class="textstyle1"><div id="container_6cf5d1d2"><div id="container_6cf5d1d2_padding" ><div class="textstyle1"><span class="textstyle2"><br/></span><div id="container_118931d5"><div id="container_118931d5_padding" ></div></div></div>
<div class="textstyle3"><div id="container_74337d4f"><div id="container_74337d4f_padding" ><div class="textstyle3"><span class="textstyle4"><br/>IREye4Task</span><span class="textstyle5"><br/><br/>Mental states, such as cognition, emotion and action (contexts) can be analyzed and predicted from the eye acquired by a close-up infrared sensitive camera</span><span class="textstyle6">. </span>  <span class="textstyle7"><br/><br/></span>
  <a href="contact.html" style="text-decoration:none"><div id="button_4a2f61ca">
    <div class="vcenterstyle1"><div class="vcenterstyle2">      <div class="textstyle1">
        <span class="textstyle8">Contact</span>
        </div>
      <div class="textstyle3">
        </div>
      </div></div>
    </div></a>
  <span class="textstyle9">   </span>
  <a href="about.html" style="text-decoration:none"><div id="button_212cc296">
    <div class="vcenterstyle1"><div class="vcenterstyle2">      <div class="textstyle1">
        <span class="textstyle8">About us</span>
        </div>
      <div class="textstyle3">
        </div>
      </div></div>
    </div></a>
</div>
<div class="textstyle1">  <span class="textstyle9"><br/><br/></span>
</div>
<div style="clear:both"></div></div></div></div>
<div style="clear:both"></div></div></div></div>
</div>  </div>
<div class="textstyle3">
<div id="container_5dc15a0d"><div class="textstyle1"><div id="container_5152054b"><div id="container_5152054b_padding" ><div class="textstyle3"><span class="textstyle2"><br/></span><span class="textstyle10">Tutorial: Introduction to eye and audio behavior computing for affect analysis in wearable contexts<br/><br/></span><div id="container_10853e17"><div id="container_10853e17_padding" ><div class="textstyle3">  <span class="textstyle11">This tutorial is held in 2023 International Conference on Affective Computing and Intelligent Interaction (ACII) at the MIT Media Lab in Cambridge, MA, USA on 10 Sept. 2023. Students and early-stage researchers in any field of study who have used or wish to use eye or speech behaviour, multimodality and/or wearables in their projects are the primary target audience. A basic machine learning/pattern recognition knowledge is a prerequisite. The tutorial content covers a wide range of topics, from general concepts to more detailed mathematical and technical aspects, benefiting both non-technical and technical audiences. This tutorial will offer fresh insights to those looking to explore this emerging field and also be valuable to researchers with experience in eye and speech behaviour or multimodal affective computing.<br/><br/></span>
</div>
<div class="textstyle1">  <a href="acii_tutorial.html" style="text-decoration:none"><div id="button_26982c48">
    <div class="vcenterstyle1"><div class="vcenterstyle2">      <div class="textstyle3">
        </div>
      <div class="textstyle1">
        <span class="textstyle12">ACII2023 Tutorial</span>
        </div>
      </div></div>
    </div></a>
</div>
<div class="textstyle3">  <span class="textstyle11"><br/></span>
</div>
<div style="clear:both"></div></div></div><div id="container_10224cb5"><div id="container_10224cb5_padding" ><div class="textstyle3"><img src="rc_images/acii_tutorial_1200x671.png" width="1200" height="671" id="img_ea3b47e" alt="" title="" /></div>
<div style="clear:both"></div></div></div><span class="textstyle2"><br/></span></div>
<div style="clear:both"></div></div></div></div>
</div><span class="textstyle2"><br/></span><div id="container_147dd5a4"><div class="textstyle1"><div id="container_7d38756a"><div id="container_7d38756a_padding" ><div class="textstyle3"><span class="textstyle2"><br/></span><span class="textstyle10">Tutorial: Introduction to eye and speech behavior computing for affect analysis in wearable contexts<br/><br/></span><div id="container_78514065"><div id="container_78514065_padding" ><div class="textstyle3">  <span class="textstyle11">This tutorial is held in the 25th ACM International Conference on Multimodal Interaction (ICMI) at Sorbonne University, Campus Pierre &amp; Marie Curie on 13 Oct. 2023. It is designed to introduce the foundational computing methods for analysing close-up infrared eye images and speech/audio signals from body-worn sensors, the theoretical (e.g. psychophysiological) basis for the relationship between sensing modalities and affect, and the latest development of models for affect analysis. It will cover eye and speech/audio behaviour analysis, statistical modelling and machine learning pipelines, as well as multimodal systems centred on eye and speech/audio behaviour for affect. Various application areas will be discussed, along with examples that illustrate the potential, challenges, and pitfalls of methodologies in wearable contexts.<br/><br/></span>
</div>
<div class="textstyle1">  <a href="icmi_tutorial.html" style="text-decoration:none"><div id="button_782e733d">
    <div class="vcenterstyle1"><div class="vcenterstyle2">      <div class="textstyle3">
        </div>
      <div class="textstyle1">
        <span class="textstyle12">ICMI2023 tutorial</span>
        </div>
      </div></div>
    </div></a>
</div>
<div class="textstyle3">  <span class="textstyle11"><br/></span>
</div>
<div style="clear:both"></div></div></div><div id="container_4a7848cd"><div id="container_4a7848cd_padding" ><div class="textstyle13"><img src="rc_images/speech.jpg" width="742" height="555" id="img_37eddc0e" alt="" title="" /></div>
<div style="clear:both"></div></div></div><span class="textstyle2"><br/></span></div>
<div style="clear:both"></div></div></div></div>
</div><span class="textstyle2"><br/></span><div id="container_564289d1"><div class="textstyle1"><div id="container_48e28962"><div id="container_48e28962_padding" ><div class="textstyle3"><span class="textstyle2"><br/></span><span class="textstyle10">Open-access dataset: IREye4Task<br/><br/></span><div id="container_9da77dd"><div id="container_9da77dd_padding" ><div class="textstyle3">  <span class="textstyle11">IREYE4TASK is an open-access dataset for wearable eye landmark detection (computer vision) and mental state analysis (signal processing and affective computing). It contains annotated landmarks on the eyelid, pupil and iris boundary on each frame from 20 partcipants' eye videos (over more than a million frames) as responses to four different task contexts (cognitve, perceputal, physical and communicative task) and two load levels of tasks, and the task context groundtruth in high level of granularity. </span>
<span class="textstyle14">It is free to download and is for research use only.</span><span class="textstyle11"><br/><br/></span></div>
<div class="textstyle1">  <a href="acii_tutorial.html" style="text-decoration:none"><div id="button_5b94d7a6">
    <div class="vcenterstyle1"><div class="vcenterstyle2">      <div class="textstyle3">
        </div>
      <div class="textstyle1">
        <span class="textstyle12">IREye4Task dataset</span>
        </div>
      </div></div>
    </div></a>
</div>
<div class="textstyle3">  <span class="textstyle11"><br/></span>
</div>
<div style="clear:both"></div></div></div><div id="container_7a349e57"><div id="container_7a349e57_padding" ><div class="textstyle13"><img src="rc_images/dataset.png" width="408" height="555" id="img_11641a92" alt="" title="" /></div>
<div style="clear:both"></div></div></div><span class="textstyle2"><br/></span></div>
<div style="clear:both"></div></div></div></div>
</div>  </div>
<div class="textstyle1">
<div id="container_7d6eadac"><div id="container_7d6eadac_padding" ><div class="textstyle3"><span class="textstyle15">Get into contact with us!<br/></span><span class="textstyle2"><br/><br/></span><div id="container_718109b3"><div id="container_718109b3_padding" ><div class="textstyle3"><div id="container_5382d0f7"><div id="container_5382d0f7_padding" ><div class="textstyle3"><span class="textstyle16">How to contact us</span><span class="textstyle2"><br/></span>  <span class="textstyle17"> </span>
<ul id="ul_7d88a215" class="ulstyle1">
<li><span class="textstyle17">Email: siyuan.chen(at)ieee.org</span></li>
<li><span class="textstyle17">Office open: every day, 08:00-17:00</span></div>
</li>
</ul>
</div></div><div id="container_1fb17606"><div id="container_1fb17606_padding" ><div class="textstyle3"><span class="textstyle16">Where to Find us</span><span class="textstyle2"><br/><br/></span><span class="textstyle18">School of EET<br/>Faculaty of Engineering and Technology<br/>University of New South Wales<br/>Kensington, NSW 2033<br/>Australia</span></div>
</div></div><div id="container_638c8d1a"><div id="container_638c8d1a_padding" ><div class="textstyle3"><span class="textstyle16">Social Media</span><span class="textstyle2"><br/><br/></span><ul id="ul_534c4641" class="ulstyle2">
<li><span class="textstyle19">Twitter</span></li>
<li><span class="textstyle19">Facebook</span></li>
<li><span class="textstyle19">Other</span></div>
</li>
</ul>
</div></div></div>
<div style="clear:both"></div></div></div></div>
<div style="clear:both"></div></div></div>  </div>
</body>
</html>